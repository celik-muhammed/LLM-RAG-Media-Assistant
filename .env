######################################################################
# TZ
# (Optional) Timezone (default: UTC)
# Default: UTC, but you can set your local zone for logs
# Timezone from https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
######################################################################
RUN_TIMEZONE_CHECK=0
# TZ=America/Edmonton
# TZ=Europe/Berlin
TZ=Europe/Istanbul

######################################################################
# Data
######################################################################
DATA_PATH=../Data/documents-with-ids.json
DATA_URL=https://huggingface.co/datasets/bitext/Bitext-media-llm-chatbot-training-dataset/resolve/main/bitext-media-llm-chatbot-training-dataset.csv

######################################################################
# Scraper defaults pdfs
######################################################################
DEFAULT_MAX_PAGES=1
SE_SLOWMO_MS=200
TIMEOUT_PDF_REQUEST=60

######################################################################
# PostgreSQL Configuration
######################################################################
# AUTHENTICATOR=authenticator
# ANON=web_anon
# SCHEMA=public
POSTGRES_DB=llm_rag
## 'localhost' or 'postgres' (docker service name)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin
POSTGRES_TABLE=conversations
POSTGRES_TABLE1=feedback

######################################################################
# Grafana Configuration
# https://grafana.com/docs/grafana/latest/setup-grafana/configure-docker/
######################################################################
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin
GRAFANA_SECRET_KEY=SECRET_KEY

######################################################################
# LLM RAG MODEL
######################################################################
## OPENAI or OLLAMA or HF
LLM_PROVIDER=OLLAMA

## Ollama: https://ollama.com/library/phi3
## Ollama doesnâ€™t need a Python client, just can be use REST requests
OLLAMA_API_KEY=ollama
## 'localhost' or 'ollama' (docker service name)
# OLLAMA_HOST=localhost
# OLLAMA_PORT=11434
## http://localhost:11434/v1 http://ollama:11434/v1
OLLAMA_BASE_URL=http://localhost:11434/v1
## generate embeddingsmodel "nomic-embed-text" http://localhost:11434/api/embeddings
## 'nomic-embed-text:latest'  https://ollama.com/library/nomic-embed-text  https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
OLLAMA_MODEL_EMBED=nomic-embed-text
## 'phi3:latest'  https://ollama.com/library/phi3  'gpt-oss-120b'  https://ollama.com/library/gpt-oss:120b
## Phi 3 Mini 128k Instruct
OLLAMA_MODEL_CHAT=phi3

## (Optional) huggingface: https://huggingface.co/models
HF_TOKEN=hf-...
HF_API_KEY=${HF_TOKEN}
## use for chat model based on huggingface.co
HF_BASE_URL=https://router.huggingface.co/v1  
## use by fastembed faiss sentence_transformers https://huggingface.co/nomic-ai/modernbert-embed-base  https://huggingface.co/nomic-ai/nomic-embed-text-v1.5  https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  https://huggingface.co/jinaai/jina-embeddings-v2-small-en
HF_MODEL_EMBED=nomic-ai/nomic-embed-text-v1.5  
## https://huggingface.co/openai/gpt-oss-120b  https://huggingface.co/openai/gpt-oss-20b  https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
## explicit define Providers (together, fireworks-ai, groq, ...)
HF_MODEL_CHAT=openai/gpt-oss-120b:together

## OpenAI: https://platform.openai.com/docs/guides/your-data#storage-requirements-and-retention-controls-per-endpoint
OPENAI_API_KEY=sk-...
## https://us.api.openai.com/ https://us.api.openai.com/
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL_EMBED=text-embedding-3-small
# gpt-4o
OPENAI_MODEL_CHAT=gpt-4o-mini

# Chunking
CHUNK_MAX_TOKENS=512
CHUNK_OVERLAP_TOKENS=64

# Feature flags
ENABLE_FAISS=false

######################################################################
# Application
######################################################################
APP_PORT=5000
